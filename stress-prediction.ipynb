{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5100130,"sourceType":"datasetVersion","datasetId":2961947}],"dockerImageVersionId":30646,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-03T13:58:53.555287Z","iopub.execute_input":"2024-04-03T13:58:53.555863Z","iopub.status.idle":"2024-04-03T13:58:54.904808Z","shell.execute_reply.started":"2024-04-03T13:58:53.555830Z","shell.execute_reply":"2024-04-03T13:58:54.903824Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/human-stress-prediction/Stress.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\n\nfrom sklearn.metrics import accuracy_score\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport gensim\nfrom gensim.models import Word2Vec\nfrom sklearn.model_selection import train_test_split\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-03T13:58:54.906326Z","iopub.execute_input":"2024-04-03T13:58:54.907164Z","iopub.status.idle":"2024-04-03T13:59:11.924115Z","shell.execute_reply.started":"2024-04-03T13:58:54.907135Z","shell.execute_reply":"2024-04-03T13:59:11.922830Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"stress_data_path = '../input/human-stress-prediction/Stress.csv'\nstress_data = pd.read_csv(stress_data_path)\nstress_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-03T13:59:11.925889Z","iopub.execute_input":"2024-04-03T13:59:11.926534Z","iopub.status.idle":"2024-04-03T13:59:12.013077Z","shell.execute_reply.started":"2024-04-03T13:59:11.926500Z","shell.execute_reply":"2024-04-03T13:59:12.011816Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"          subreddit post_id sentence_range  \\\n0              ptsd  8601tu       (15, 20)   \n1        assistance  8lbrx9         (0, 5)   \n2              ptsd  9ch1zh       (15, 20)   \n3     relationships  7rorpp        [5, 10]   \n4  survivorsofabuse  9p2gbc         [0, 5]   \n\n                                                text  label  confidence  \\\n0  He said he had not felt that way before, sugge...      1         0.8   \n1  Hey there r/assistance, Not sure if this is th...      0         1.0   \n2  My mom then hit me with the newspaper and it s...      1         0.8   \n3  until i met my new boyfriend, he is amazing, h...      1         0.6   \n4  October is Domestic Violence Awareness Month a...      1         0.8   \n\n   social_timestamp  \n0        1521614353  \n1        1527009817  \n2        1535935605  \n3        1516429555  \n4        1539809005  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>subreddit</th>\n      <th>post_id</th>\n      <th>sentence_range</th>\n      <th>text</th>\n      <th>label</th>\n      <th>confidence</th>\n      <th>social_timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ptsd</td>\n      <td>8601tu</td>\n      <td>(15, 20)</td>\n      <td>He said he had not felt that way before, sugge...</td>\n      <td>1</td>\n      <td>0.8</td>\n      <td>1521614353</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>assistance</td>\n      <td>8lbrx9</td>\n      <td>(0, 5)</td>\n      <td>Hey there r/assistance, Not sure if this is th...</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1527009817</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ptsd</td>\n      <td>9ch1zh</td>\n      <td>(15, 20)</td>\n      <td>My mom then hit me with the newspaper and it s...</td>\n      <td>1</td>\n      <td>0.8</td>\n      <td>1535935605</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>relationships</td>\n      <td>7rorpp</td>\n      <td>[5, 10]</td>\n      <td>until i met my new boyfriend, he is amazing, h...</td>\n      <td>1</td>\n      <td>0.6</td>\n      <td>1516429555</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>survivorsofabuse</td>\n      <td>9p2gbc</td>\n      <td>[0, 5]</td>\n      <td>October is Domestic Violence Awareness Month a...</td>\n      <td>1</td>\n      <td>0.8</td>\n      <td>1539809005</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"longest_post = max(stress_data.text, key=len)\nlen(longest_post.split()) #nr of words in the longest post","metadata":{"execution":{"iopub.status.busy":"2024-04-03T13:59:12.016873Z","iopub.execute_input":"2024-04-03T13:59:12.017298Z","iopub.status.idle":"2024-04-03T13:59:12.026732Z","shell.execute_reply.started":"2024-04-03T13:59:12.017266Z","shell.execute_reply":"2024-04-03T13:59:12.025690Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"292"},"metadata":{}}]},{"cell_type":"code","source":"post_lengths = stress_data.text.apply(len) #lengths of all posts\n\npost_lengths.mean() #avg length\nstress_data.text","metadata":{"execution":{"iopub.status.busy":"2024-04-03T13:59:12.028003Z","iopub.execute_input":"2024-04-03T13:59:12.028422Z","iopub.status.idle":"2024-04-03T13:59:12.048895Z","shell.execute_reply.started":"2024-04-03T13:59:12.028328Z","shell.execute_reply":"2024-04-03T13:59:12.047804Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"0       He said he had not felt that way before, sugge...\n1       Hey there r/assistance, Not sure if this is th...\n2       My mom then hit me with the newspaper and it s...\n3       until i met my new boyfriend, he is amazing, h...\n4       October is Domestic Violence Awareness Month a...\n                              ...                        \n2833    * Her, a week ago: Precious, how are you? (I i...\n2834    I don't have the ability to cope with it anymo...\n2835    In case this is the first time you're reading ...\n2836    Do you find this normal? They have a good rela...\n2837    I was talking to my mom this morning and she s...\nName: text, Length: 2838, dtype: object"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Text Preprocessing**","metadata":{}},{"cell_type":"code","source":"\n#removing special characters\ndef remove_spec(text):\n    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n\nclean_posts = list(map(remove_spec, stress_data.text))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-03T13:59:12.050589Z","iopub.execute_input":"2024-04-03T13:59:12.051339Z","iopub.status.idle":"2024-04-03T13:59:12.112174Z","shell.execute_reply.started":"2024-04-03T13:59:12.051310Z","shell.execute_reply":"2024-04-03T13:59:12.110833Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#spellchecking???","metadata":{"execution":{"iopub.status.busy":"2024-04-03T13:59:12.113957Z","iopub.execute_input":"2024-04-03T13:59:12.114411Z","iopub.status.idle":"2024-04-03T13:59:12.119903Z","shell.execute_reply.started":"2024-04-03T13:59:12.114343Z","shell.execute_reply":"2024-04-03T13:59:12.118668Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#stopwords are words that mostly have grammatical meaning, which we can exclude\n\n# List of sentences to be vectorized\nlines = clean_posts\n\n# Removing stop words\nstop_words = set(stopwords.words('english'))\nlines_without_stopwords = []\nfor line in lines:\n    words = line.split()\n    filtered_words = [word for word in words if word.lower() not in stop_words]\n    lines_without_stopwords.append(' '.join(filtered_words))\n\nlines = lines_without_stopwords\n\n# Lemmatization\n#wordnet_lemmatizer = WordNetLemmatizer()\n#lines_with_lemmas = []\n#for line in lines:\n #   words = line.split()\n  #  lemmatized_words = [wordnet_lemmatizer.lemmatize(word) for word in words]\n   # lines_with_lemmas.append(' '.join(lemmatized_words))\n\n#lines = lines_with_lemmas\nclean_posts = lines","metadata":{"execution":{"iopub.status.busy":"2024-04-03T13:59:12.121068Z","iopub.execute_input":"2024-04-03T13:59:12.121470Z","iopub.status.idle":"2024-04-03T13:59:12.221547Z","shell.execute_reply.started":"2024-04-03T13:59:12.121440Z","shell.execute_reply":"2024-04-03T13:59:12.220418Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# **Splitting Data**","metadata":{}},{"cell_type":"code","source":"X = clean_posts #attributes according to which we classify\ny = stress_data.label #class\n\n# split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, random_state=4)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-03T14:10:46.306224Z","iopub.execute_input":"2024-04-03T14:10:46.306674Z","iopub.status.idle":"2024-04-03T14:10:46.315940Z","shell.execute_reply.started":"2024-04-03T14:10:46.306640Z","shell.execute_reply":"2024-04-03T14:10:46.314430Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"X = clean_posts #attributes according to which we classify\ny = stress_data.label #class\n# Convert X and y to NumPy arrays\n\n# Initialize the stratified k-fold cross-validation object\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\nsplit_data = skf.split(X, y)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T14:15:09.113581Z","iopub.execute_input":"2024-04-03T14:15:09.114001Z","iopub.status.idle":"2024-04-03T14:15:09.120869Z","shell.execute_reply.started":"2024-04-03T14:15:09.113971Z","shell.execute_reply":"2024-04-03T14:15:09.119530Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"# **Bag of Words**","metadata":{}},{"cell_type":"code","source":"# create the transform\ntf_vectorizer=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,1))\n\n# tokenize and build vocab\nbow_train = tf_vectorizer.fit_transform(X_train)\n# encode document\nbow_test = tf_vectorizer.transform(X_test)\n# summarize\n#print(sorted(vectorizer.vocabulary_))\nprint('BOW train: ',bow_train.shape)\nprint('BOW test: ',bow_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T14:05:47.593730Z","iopub.execute_input":"2024-04-03T14:05:47.594130Z","iopub.status.idle":"2024-04-03T14:05:47.771147Z","shell.execute_reply.started":"2024-04-03T14:05:47.594099Z","shell.execute_reply":"2024-04-03T14:05:47.769989Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"BOW train:  (2128, 5360)\nBOW test:  (710, 5360)\n","output_type":"stream"}]},{"cell_type":"code","source":"print(len(vectorizer.idf_))\n\n# summarize encoded vector\nprint(vectorized_posts[0].shape)\nprint(vectorized_posts[0].toarray())\n","metadata":{"execution":{"iopub.status.busy":"2024-04-03T13:59:12.455402Z","iopub.execute_input":"2024-04-03T13:59:12.455722Z","iopub.status.idle":"2024-04-03T13:59:12.814247Z","shell.execute_reply.started":"2024-04-03T13:59:12.455695Z","shell.execute_reply":"2024-04-03T13:59:12.812729Z"},"trusted":true},"execution_count":12,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mvectorizer\u001b[49m\u001b[38;5;241m.\u001b[39midf_))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# summarize encoded vector\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(vectorized_posts[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n","\u001b[0;31mNameError\u001b[0m: name 'vectorizer' is not defined"],"ename":"NameError","evalue":"name 'vectorizer' is not defined","output_type":"error"}]},{"cell_type":"code","source":"#print(len(vectorized_posts))\n#print(len(stress_data))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-03T13:59:12.815349Z","iopub.status.idle":"2024-04-03T13:59:12.815764Z","shell.execute_reply.started":"2024-04-03T13:59:12.815574Z","shell.execute_reply":"2024-04-03T13:59:12.815590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Bag of N-Grams**","metadata":{}},{"cell_type":"code","source":"# create the transform\ntf_vectorizer2 = TfidfVectorizer(analyzer='word', ngram_range=(7, 7)) #instead of separate words treat them as 7-grams, groups of len 7\n# tokenize and build vocab\nbon_train = tf_vectorizer2.fit_transform(X_train)\n\n# encode document\nbon_test = tf_vectorizer2.transform(X_test)\n# summarize\n#print(sorted(vectorizer.vocabulary_))\nprint('BON train: ',bow_train.shape)\nprint('BON test: ',bow_test.shape)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-03T14:05:51.029054Z","iopub.execute_input":"2024-04-03T14:05:51.029474Z","iopub.status.idle":"2024-04-03T14:05:51.511641Z","shell.execute_reply.started":"2024-04-03T14:05:51.029442Z","shell.execute_reply":"2024-04-03T14:05:51.510403Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"BON train:  (2128, 5360)\nBON test:  (710, 5360)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Word2Vec**","metadata":{}},{"cell_type":"code","source":"vectorized_w2v = [gensim.utils.simple_preprocess(text) for text in clean_posts]\nlen(vectorized_w2v)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-03T13:59:40.878715Z","iopub.execute_input":"2024-04-03T13:59:40.879144Z","iopub.status.idle":"2024-04-03T13:59:41.226441Z","shell.execute_reply.started":"2024-04-03T13:59:40.879113Z","shell.execute_reply":"2024-04-03T13:59:41.225438Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"2838"},"metadata":{}}]},{"cell_type":"code","source":"# initialize the model\n#w2v = Word2Vec(clean_posts, min_count=1, vector_size=3)\n\n# Model parameters\nw2v=gensim.models.Word2Vec(window=5, min_count=1,vector_size=200, workers=4, sg=0)\n\n# train the model\nw2v.build_vocab(vectorized_w2v)\nw2v.train(vectorized_w2v, total_examples=w2v.corpus_count, epochs=w2v.epochs)\n1\n# save the trained model\n#w2v.save(\"./responses.model\")\n#print(w2v.wv['plastic']) \n\n#print((w2v.wv.key_to_index)) \n","metadata":{"execution":{"iopub.status.busy":"2024-04-03T13:59:42.192154Z","iopub.execute_input":"2024-04-03T13:59:42.192569Z","iopub.status.idle":"2024-04-03T13:59:43.380657Z","shell.execute_reply.started":"2024-04-03T13:59:42.192540Z","shell.execute_reply":"2024-04-03T13:59:43.379603Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}]},{"cell_type":"code","source":"# Initialize an empty list to store post vectors\npost_vectors = []\n\n# Iterate through each tokenized list of words (each post)\nfor post in vectorized_w2v:\n    # Initialize an empty list to store word vectors for this post\n    word_vectors_post = []\n    \n    # Iterate through the words in the post\n    for word in post:\n        # Check if the word exists in the vocabulary\n            # Retrieve the word vector from the model\n            word_vector = w2v.wv[word]\n            # Append the word vector to the list of word vectors for this post\n            word_vectors_post.append(word_vector)\n    \n    \n        # Calculate the average of word vectors for this post\n    post_vector = np.sum(word_vectors_post, axis=0)  # You can also use np.sum() instead of np.mean() to get the sum\n    # Append the post vector to the list of post vectors\n    post_vectors.append(post_vector)\n    \n\n# Convert the list of post vectors to a numpy array\npost_vectors_array = np.array(post_vectors)\n\nvectorized_posts = post_vectors_array\n# Now, post_vectors_array contains the vector representations for each post,\n# where each row corresponds to a post and each column corresponds to a dimension of the post vector\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-03T13:59:44.023829Z","iopub.execute_input":"2024-04-03T13:59:44.024201Z","iopub.status.idle":"2024-04-03T13:59:44.503705Z","shell.execute_reply.started":"2024-04-03T13:59:44.024174Z","shell.execute_reply":"2024-04-03T13:59:44.502584Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# **Naive Bayes**\n\n* 1. naive bayes with stratified kfold splitting and bag of words 59% \n* nb with train test split and bag of ngrams 51%","metadata":{}},{"cell_type":"code","source":"# Initialize the Gaussian Naive Bayes classifier\ngnb = GaussianNB()\n\n# Initialize variables to store the total accuracy across all folds\ntotal_accuracy = 0.0\n\n# Iterate over the folds\nfor train_index, test_index in split_data:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    # Fit the model on the training data\n    gnb.fit(X_train, y_train)\n\n    # Make predictions on the test data\n    y_pred = gnb.predict(X_test)\n\n    # Calculate accuracy for this fold\n    fold_accuracy = accuracy_score(y_test, y_pred)\n\n    # Accumulate accuracy for all folds\n    total_accuracy += fold_accuracy\n\n    # Print accuracy for this fold\n    print(\"Accuracy for this fold:\", fold_accuracy)\n\n# Calculate average accuracy across all folds\naverage_accuracy = total_accuracy / skf.n_splits\n\nprint(\"Average Accuracy:\", average_accuracy)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-03T14:16:35.847310Z","iopub.execute_input":"2024-04-03T14:16:35.848371Z","iopub.status.idle":"2024-04-03T14:16:35.919180Z","shell.execute_reply.started":"2024-04-03T14:16:35.848317Z","shell.execute_reply":"2024-04-03T14:16:35.917392Z"},"trusted":true},"execution_count":56,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[56], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Iterate over the folds\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_index, test_index \u001b[38;5;129;01min\u001b[39;00m split_data:\n\u001b[0;32m----> 9\u001b[0m     X_train, X_test \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[train_index], X\u001b[38;5;241m.\u001b[39mshape[test_index]\n\u001b[1;32m     10\u001b[0m     y_train, y_test \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[train_index], y\u001b[38;5;241m.\u001b[39mshape[test_index]\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Fit the model on the training data\u001b[39;00m\n","\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"],"ename":"AttributeError","evalue":"'list' object has no attribute 'shape'","output_type":"error"}]},{"cell_type":"code","source":"# initialize the naive bayes classifier\ngnb1 = GaussianNB()\ngnb2 = GaussianNB()\n\n#fitting the nb for bag of words\ngnb_bow=gnb1.fit(bow_train.toarray(), y_train)\nprint(gnb_bow)\n#fitting the nb for tfidf features\ngnb_bon=gnb2.fit(bon_train.toarray(), y_train)\nprint(gnb_bon)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-03T14:06:48.096543Z","iopub.execute_input":"2024-04-03T14:06:48.096940Z","iopub.status.idle":"2024-04-03T14:06:51.090569Z","shell.execute_reply.started":"2024-04-03T14:06:48.096911Z","shell.execute_reply":"2024-04-03T14:06:51.089481Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"GaussianNB()\nGaussianNB()\n","output_type":"stream"}]},{"cell_type":"code","source":"#Predicting the model for bag of words\ngnb_bow_predict = gnb_bow.predict(bow_test.toarray())\n\n#Predicting the model for bag of ngrams\ngnb_bon_predict = gnb_bon.predict(bon_test.toarray())\n","metadata":{"execution":{"iopub.status.busy":"2024-04-03T14:06:55.435908Z","iopub.execute_input":"2024-04-03T14:06:55.436602Z","iopub.status.idle":"2024-04-03T14:06:56.405324Z","shell.execute_reply.started":"2024-04-03T14:06:55.436564Z","shell.execute_reply":"2024-04-03T14:06:56.404241Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"#Accuracy score for bag of words\ngnb_bow_score=accuracy_score(y_test,gnb_bow_predict)\nprint(\"gnb_bow_score :\",gnb_bow_score)\n#Accuracy score for tfidf features\ngnb_bon_score=accuracy_score(y_test,gnb_bon_predict)\nprint(\"gnb_bon_score :\",gnb_bon_score)","metadata":{"execution":{"iopub.status.busy":"2024-04-03T14:07:01.815257Z","iopub.execute_input":"2024-04-03T14:07:01.815686Z","iopub.status.idle":"2024-04-03T14:07:01.823647Z","shell.execute_reply.started":"2024-04-03T14:07:01.815656Z","shell.execute_reply":"2024-04-03T14:07:01.822826Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"gnb_bow_score : 0.5309859154929577\ngnb_bon_score : 0.48169014084507045\n","output_type":"stream"}]},{"cell_type":"code","source":"# calculate the number of mislabeled points\nnum_mislabeled = (y_test != y_pred).sum()\naccuracy = num_mislabeled/len(X_test)*100.0\n \n\nprint(\"Number of mislabeled points out of a total %d points: %d\" % (len(X_test), num_mislabeled))\nprint(\"Accuracy: %d \" % (accuracy))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-03T14:07:22.835644Z","iopub.execute_input":"2024-04-03T14:07:22.836022Z","iopub.status.idle":"2024-04-03T14:07:22.874683Z","shell.execute_reply.started":"2024-04-03T14:07:22.835995Z","shell.execute_reply":"2024-04-03T14:07:22.873191Z"},"trusted":true},"execution_count":41,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[41], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# calculate the number of mislabeled points\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m num_mislabeled \u001b[38;5;241m=\u001b[39m (y_test \u001b[38;5;241m!=\u001b[39m \u001b[43my_pred\u001b[49m)\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m      3\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m num_mislabeled\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(X_test)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100.0\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of mislabeled points out of a total \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m points: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mlen\u001b[39m(X_test), num_mislabeled))\n","\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"],"ename":"NameError","evalue":"name 'y_pred' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.naive_bayes import GaussianNB\n\nX = vectorized_posts #attributes according to which we classify\ny = stress_data.label #class\n\n# split data into training and testing sets\n#X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, random_state=0)\n\nn_splits = 5\n\n# Initialize the StratifiedKFold object\nstratified_kfold = StratifiedKFold(n_splits=n_splits,shuffle=True, random_state=0)\n# initialize the naive bayes classifier\ngnb = GaussianNB()\nacc = 0\nx = 0\nn =0 \n# Iterate over the splits\nfor train_index, test_index in stratified_kfold.split(X, y):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    # fit the model on the training data\n    gnb.fit(X_train.toarray(), y_train)\n    #make a prediction on the test data\n    y_pred = gnb.predict(X_test.toarray())\n    # calculate the number of mislabeled points\n    num_mislabeled = (y_test != y_pred).sum()\n    n += num_mislabeled\n    x+=X_test.shape[0]\n    accuracy = num_mislabeled/X_test.shape[0]*100.0\n    acc+=accuracy\n    \n\nprint(\"Number of mislabeled points out of a total %d points: %d\" % (x, n))\nprint(\"Accuracy: %d \" % (acc/5))","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}